# ============================================================
# APP SURVEILLANCE & PR√âDICTION ROUGEOLE - VERSION 3.0
# PARTIE 1/6 - IMPORTS ET CONFIGURATION
# ============================================================

import streamlit as st
import pandas as pd
import numpy as np
import geopandas as gpd
from datetime import datetime, timedelta
import requests
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
import ee
import json
import folium
from folium.plugins import HeatMap, MarkerCluster
from streamlit_folium import st_folium
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from io import BytesIO
import zipfile
import tempfile
import os
from shapely.geometry import shape
import warnings
warnings.filterwarnings('ignore')

# Configuration Streamlit
st.set_page_config(
    page_title="Surveillance Rougeole Multi-pays",
    layout="wide",
    page_icon="ü¶†",
    initial_sidebar_state="expanded"
)

# CSS personnalis√©
st.markdown("""
<style>
.metric-card{background-color:#f0f2f6;padding:15px;border-radius:10px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
.high-risk{background-color:#ffebee;color:#c62828;font-weight:bold;padding:5px;border-radius:3px}
.medium-risk{background-color:#fff3e0;color:#ef6c00;padding:5px;border-radius:3px}
.low-risk{background-color:#e8f5e9;color:#2e7d32;padding:5px;border-radius:3px}
.stButton>button{width:100%}
h1{color:#d32f2f}
.info-box{background-color:#e3f2fd;padding:10px;border-left:4px solid #2196f3;margin:10px 0}
.model-hint{background-color:#fff9c4;padding:8px;border-radius:5px;font-size:0.9em;margin:5px 0}
.weight-box{background-color:#e8f5e9;padding:10px;border-radius:5px;margin:10px 0;border-left:4px solid #4caf50}
</style>
""", unsafe_allow_html=True)

st.title("ü¶† Dashboard de Surveillance et Pr√©diction - Rougeole")
st.markdown("### Analyse √©pid√©miologique et mod√©lisation pr√©dictive par semaines √©pid√©miologiques")

# Mapping pays ISO3
PAYS_ISO3_MAP = {
    "Niger": "ner",
    "Burkina Faso": "bfa",
    "Mali": "mli",
    "Mauritanie": "mrt"
}

# Initialisation Google Earth Engine
@st.cache_resource
def init_gee():
    try:
        key_dict = json.loads(st.secrets["GEE_SERVICE_ACCOUNT"])
        credentials = ee.ServiceAccountCredentials(
            key_dict["client_email"],
            key_data=json.dumps(key_dict)
        )
        ee.Initialize(credentials)
        return True
    except:
        try:
            ee.Initialize()
            return True
        except:
            return False

gee_ok = init_gee()
if gee_ok:
    st.sidebar.success("‚úì GEE connect√©")

# Session state
if 'pays_precedent' not in st.session_state:
    st.session_state.pays_precedent = None
if 'sa_gdf_cache' not in st.session_state:
    st.session_state.sa_gdf_cache = None

# Configuration Sidebar
st.sidebar.header("üìÇ Configuration de l'Analyse")

# Mode d√©mo
st.sidebar.subheader("üéØ Mode d'utilisation")
mode_demo = st.sidebar.radio(
    "Choisissez votre mode",
    ["üìä Donn√©es r√©elles", "üß™ Mode d√©mo (donn√©es simul√©es)"],
    help="Mode d√©mo : g√©n√®re automatiquement des donn√©es fictives pour tester l'application"
)

# Aires de sant√©
st.sidebar.subheader("üó∫Ô∏è Aires de Sant√©")
option_aire = st.sidebar.radio(
    "Source des donn√©es g√©ographiques",
    ["Fichier local (ao_hlthArea.zip)", "Upload personnalis√©"],
    key='option_aire'
)

pays_selectionne = None
iso3_pays = None

if option_aire == "Fichier local (ao_hlthArea.zip)":
    pays_selectionne = st.sidebar.selectbox(
        "üåç S√©lectionner le pays",
        list(PAYS_ISO3_MAP.keys()),
        key='pays_select'
    )
    iso3_pays = PAYS_ISO3_MAP[pays_selectionne]
    
    pays_change = (st.session_state.pays_precedent != pays_selectionne)
    if pays_change:
        st.session_state.pays_precedent = pays_selectionne
        st.session_state.sa_gdf_cache = None
        st.rerun()

upload_file = None
if option_aire == "Upload personnalis√©":
    upload_file = st.sidebar.file_uploader(
        "Charger un fichier g√©ographique",
        type=["shp", "geojson", "zip"],
        help="Format : Shapefile ou GeoJSON avec colonnes 'iso3' et 'health_area'"
    )

# Donn√©es √©pid√©miologiques
st.sidebar.subheader("üìä Donn√©es √âpid√©miologiques")

if mode_demo == "üß™ Mode d√©mo (donn√©es simul√©es)":
    option_linelist = "Donn√©es fictives (test)"
    linelist_file = None
    vaccination_file = None
    st.sidebar.info("üìä Mode d√©mo activ√© - Donn√©es simul√©es")
else:
    linelist_file = st.sidebar.file_uploader(
        "üìã Linelists rougeole (CSV)",
        type=["csv"],
        help="Format : health_area, Semaine_Epi, Cas_Total OU Date_Debut_Eruption, Aire_Sante..."
    )
    
    vaccination_file = st.sidebar.file_uploader(
        "üíâ Couverture vaccinale (CSV - optionnel)",
        type=["csv"],
        help="Format : health_area, Taux_Vaccination (en %)"
    )

# P√©riode d'analyse
st.sidebar.subheader("üìÖ P√©riode d'Analyse")
col1, col2 = st.sidebar.columns(2)
with col1:
    start_date = st.date_input(
        "Date d√©but",
        value=datetime(2024, 1, 1),
        key='start_date'
    )
with col2:
    end_date = st.date_input(
        "Date fin",
        value=datetime.today(),
        key='end_date'
    )

# Param√®tres de pr√©diction
st.sidebar.subheader("üîÆ Param√®tres de Pr√©diction")
pred_mois = st.sidebar.slider(
    "P√©riode de pr√©diction (mois)",
    min_value=1,
    max_value=12,
    value=3,
    help="Nombre de mois √† pr√©dire apr√®s la derni√®re semaine de donn√©es"
)
n_weeks_pred = pred_mois * 4

st.sidebar.info(f"üìÜ Pr√©diction sur **{n_weeks_pred} semaines √©pid√©miologiques** (~{pred_mois} mois)")

# Choix du mod√®le
st.sidebar.subheader("ü§ñ Mod√®le de Pr√©diction")

modele_choisi = st.sidebar.selectbox(
    "Choisissez votre algorithme",
    [
        "GradientBoosting (Recommand√©)",
        "RandomForest",
        "Ridge Regression",
        "Lasso Regression",
        "Decision Tree"
    ],
    help="S√©lectionnez l'algorithme de machine learning pour la pr√©diction"
)

# Hints pour chaque mod√®le
model_hints = {
    "GradientBoosting (Recommand√©)": "üéØ **Gradient Boosting** : Tr√®s performant pour les s√©ries temporelles. Combine plusieurs mod√®les faibles pour cr√©er un mod√®le fort. Excellent pour capturer les relations non-lin√©aires. Recommand√© pour la surveillance √©pid√©miologique.",
    "RandomForest": "üå≥ **Random Forest** : Ensemble d'arbres de d√©cision. Robuste aux valeurs aberrantes et aux donn√©es manquantes. Bon pour les interactions complexes entre variables.",
    "Ridge Regression": "üìä **Ridge Regression** : R√©gression lin√©aire avec r√©gularisation L2. Simple et rapide. Id√©al pour relations lin√©aires. Moins performant sur donn√©es non-lin√©aires.",
    "Lasso Regression": "üéØ **Lasso Regression** : R√©gularisation L1 avec s√©lection automatique des variables. Utile quand beaucoup de variables peu importantes. Simplifie le mod√®le.",
    "Decision Tree": "üå≤ **Decision Tree** : Arbre de d√©cision unique. Simple √† interpr√©ter mais risque de sur-apprentissage. Moins robuste que les m√©thodes d'ensemble."
}

st.sidebar.markdown(f'<div class="model-hint">{model_hints[modele_choisi]}</div>', unsafe_allow_html=True)

# ========== SYST√àME HYBRIDE D'IMPORTANCE DES VARIABLES ==========
st.sidebar.subheader("‚öñÔ∏è Importance des Variables")

mode_importance = st.sidebar.radio(
    "Mode de pond√©ration",
    ["ü§ñ Automatique (ML)", "üë®‚Äç‚öïÔ∏è Manuel (Expert)"],
    help="Automatique : calcul√© par le mod√®le ML | Manuel : poids d√©finis par expertise √©pid√©miologique"
)

poids_manuels = {}
poids_normalises = {}

if mode_importance == "üë®‚Äç‚öïÔ∏è Manuel (Expert)":
    with st.sidebar.expander("‚öôÔ∏è Configurer les poids", expanded=True):
        st.markdown("**D√©finissez l'importance de chaque groupe de variables**")
        st.caption("Les poids seront automatiquement normalis√©s pour totaliser 100%")
        
        poids_manuels["Historique_Cas"] = st.slider(
            "üìà Historique des cas (lags)",
            min_value=0,
            max_value=100,
            value=40,
            step=5,
            help="Importance des cas pass√©s (4 derni√®res semaines)"
        )
        
        poids_manuels["Vaccination"] = st.slider(
            "üíâ Couverture vaccinale",
            min_value=0,
            max_value=100,
            value=35,
            step=5,
            help="Importance du taux de vaccination et non-vaccin√©s"
        )
        
        poids_manuels["Demographie"] = st.slider(
            "üë• D√©mographie",
            min_value=0,
            max_value=100,
            value=15,
            step=5,
            help="Importance de la population et densit√©"
        )
        
        poids_manuels["Urbanisation"] = st.slider(
            "üèôÔ∏è Urbanisation",
            min_value=0,
            max_value=100,
            value=8,
            step=2,
            help="Importance du type d'habitat (urbain/rural)"
        )
        
        poids_manuels["Climat"] = st.slider(
            "üå°Ô∏è Facteurs climatiques",
            min_value=0,
            max_value=100,
            value=2,
            step=1,
            help="Importance de la temp√©rature, humidit√©, saison"
        )
        
        # Calculer le total et normaliser
        total_poids = sum(poids_manuels.values())
        
        if total_poids > 0:
            for key in poids_manuels:
                poids_normalises[key] = poids_manuels[key] / total_poids
        
        # Afficher le r√©sum√©
        st.markdown("---")
        st.markdown("**üìä R√©partition normalis√©e :**")
        for key, value in poids_normalises.items():
            st.markdown(f"‚Ä¢ {key} : **{value*100:.1f}%**")
        
        if abs(total_poids - 100) > 5:
            st.info(f"‚ÑπÔ∏è Total brut : {total_poids}% ‚Üí Normalis√© √† 100%")
else:
    st.sidebar.info("Le mod√®le ML calculera automatiquement l'importance optimale de chaque variable")

# Seuils d'alerte
st.sidebar.subheader("‚öôÔ∏è Seuils d'Alerte")
with st.sidebar.expander("Configurer les seuils", expanded=False):
    seuil_baisse = st.slider(
        "Seuil de baisse significative (%)",
        min_value=10,
        max_value=90,
        value=75,
        step=5,
        help="Afficher les aires avec baisse ‚â• X% par rapport √† la moyenne"
    )
    seuil_hausse = st.slider(
        "Seuil de hausse significative (%)",
        min_value=10,
        max_value=200,
        value=50,
        step=10,
        help="Afficher les aires avec hausse ‚â• X% par rapport √† la moyenne"
    )
    seuil_alerte_epidemique = st.number_input(
        "Seuil d'alerte √©pid√©mique (cas/semaine)",
        min_value=1,
        max_value=100,
        value=5,
        help="Nombre de cas par semaine d√©clenchant une alerte"
    )

# Fonctions de chargement g√©ographique
@st.cache_data
def load_health_areas_from_zip(zip_path, iso3_filter):
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            with zipfile.ZipFile(zip_path, 'r') as z:
                z.extractall(tmpdir)
            
            shp_files = [f for f in os.listdir(tmpdir) if f.endswith('.shp')]
            if not shp_files:
                raise ValueError("Aucun fichier .shp trouv√© dans le ZIP")
            
            shp_path = os.path.join(tmpdir, shp_files[0])
            gdf_full = gpd.read_file(shp_path)
            
            iso3_col = None
            for col in ['iso3', 'ISO3', 'iso_code', 'ISO_CODE', 'country_iso', 'COUNTRY_ISO']:
                if col in gdf_full.columns:
                    iso3_col = col
                    break
            
            if iso3_col is None:
                st.warning(f"‚ö†Ô∏è Colonne ISO3 non trouv√©e. Colonnes : {list(gdf_full.columns)}")
                return gpd.GeoDataFrame()
            
            gdf = gdf_full[gdf_full[iso3_col] == iso3_filter].copy()
            
            if gdf.empty:
                st.warning(f"‚ö†Ô∏è Aucune aire de sant√© pour {iso3_filter}")
                return gpd.GeoDataFrame()
            
            name_col = None
            for col in ['health_area', 'HEALTH_AREA', 'name_fr', 'name', 'NAME', 'nom', 'NOM', 'aire_sante']:
                if col in gdf.columns:
                    name_col = col
                    break
            
            if name_col:
                gdf['health_area'] = gdf[name_col]
            else:
                gdf['health_area'] = [f"Aire_{i+1}" for i in range(len(gdf))]
            
            gdf = gdf[gdf.geometry.is_valid]
            
            if gdf.crs is None:
                gdf.set_crs("EPSG:4326", inplace=True)
            elif gdf.crs.to_epsg() != 4326:
                gdf = gdf.to_crs("EPSG:4326")
            
            return gdf
            
    except Exception as e:
        st.error(f"‚ùå Erreur ZIP : {e}")
        return gpd.GeoDataFrame()

def load_shapefile_from_upload(upload_file):
    try:
        if upload_file.name.endswith('.zip'):
            with tempfile.TemporaryDirectory() as tmpdir:
                zip_path = os.path.join(tmpdir, 'upload.zip')
                with open(zip_path, 'wb') as f:
                    f.write(upload_file.getvalue())
                
                with zipfile.ZipFile(zip_path, 'r') as z:
                    z.extractall(tmpdir)
                    shp_files = [f for f in os.listdir(tmpdir) if f.endswith('.shp')]
                    if shp_files:
                        gdf = gpd.read_file(os.path.join(tmpdir, shp_files[0]))
                    else:
                        raise ValueError("Aucun .shp trouv√©")
        else:
            gdf = gpd.read_file(upload_file)
        
        if "health_area" not in gdf.columns:
            for col in ["health_area", "HEALTH_AREA", "name_fr", "name", "NAME", "nom", "NOM"]:
                if col in gdf.columns:
                    gdf["health_area"] = gdf[col]
                    break
            else:
                gdf["health_area"] = [f"Aire_{i}" for i in range(len(gdf))]
        
        gdf = gdf[gdf.geometry.is_valid]
        
        if gdf.crs is None:
            gdf.set_crs("EPSG:4326", inplace=True)
        elif gdf.crs.to_epsg() != 4326:
            gdf = gdf.to_crs("EPSG:4326")
        
        return gdf
        
    except Exception as e:
        st.error(f"‚ùå Erreur lecture : {e}")
        return gpd.GeoDataFrame()

# ============================================================
# PARTIE 2/6 - CHARGEMENT AIRES DE SANT√â ET DONN√âES DE CAS
# ============================================================

# Chargement des aires de sant√©
if st.session_state.sa_gdf_cache is not None and option_aire == "Fichier local (ao_hlthArea.zip)":
    sa_gdf = st.session_state.sa_gdf_cache
    st.sidebar.success(f"‚úì {len(sa_gdf)} aires charg√©es (cache)")
else:
    with st.spinner(f"üîÑ Chargement des aires de sant√©..."):
        if option_aire == "Fichier local (ao_hlthArea.zip)":
            zip_path = os.path.join("data", "ao_hlthArea.zip")
            if not os.path.exists(zip_path):
                st.error(f"‚ùå Fichier non trouv√© : {zip_path}")
                st.info("üìÅ Placez 'ao_hlthArea.zip' dans le dossier 'data/'")
                st.stop()
            
            sa_gdf = load_health_areas_from_zip(zip_path, iso3_pays)
            
            if sa_gdf.empty:
                st.error(f"‚ùå Impossible de charger {pays_selectionne} ({iso3_pays})")
                st.stop()
            else:
                st.sidebar.success(f"‚úì {len(sa_gdf)} aires charg√©es ({iso3_pays})")
                st.session_state.sa_gdf_cache = sa_gdf
                
        elif option_aire == "Upload personnalis√©":
            if upload_file is None:
                st.warning("‚ö†Ô∏è Veuillez uploader un fichier")
                st.stop()
            else:
                sa_gdf = load_shapefile_from_upload(upload_file)
                if sa_gdf.empty:
                    st.error("‚ùå Fichier invalide")
                    st.stop()
                else:
                    st.sidebar.success(f"‚úì {len(sa_gdf)} aires charg√©es")
                    st.session_state.sa_gdf_cache = sa_gdf

if sa_gdf.empty or sa_gdf is None:
    st.error("‚ùå Aucune aire charg√©e")
    st.stop()

# G√©n√©ration de donn√©es fictives
@st.cache_data
def generate_dummy_linelists(_sa_gdf, n=500, start=None, end=None):
    np.random.seed(42)
    
    if start is None:
        start = datetime(2024, 1, 1)
    if end is None:
        end = datetime.today()
    
    delta_days = (end - start).days
    dates = pd.to_datetime(start) + pd.to_timedelta(
        np.random.exponential(scale=delta_days/3, size=n).clip(0, delta_days).astype(int),
        unit="D"
    )
    
    df = pd.DataFrame({
        "ID_Cas": range(1, n+1),
        "Date_Debut_Eruption": dates,
        "Date_Notification": dates + pd.to_timedelta(np.random.poisson(3, n), unit="D"),
        "Aire_Sante": np.random.choice(_sa_gdf["health_area"].unique(), n),
        "Age_Mois": np.random.gamma(shape=2, scale=30, size=n).clip(6, 180).astype(int),
        "Statut_Vaccinal": np.random.choice(["Oui", "Non"], n, p=[0.55, 0.45]),
        "Sexe": np.random.choice(["M", "F"], n),
        "Issue": np.random.choice(["Gu√©ri", "D√©c√©d√©", "Inconnu"], n, p=[0.92, 0.03, 0.05])
    })
    
    return df

@st.cache_data
def generate_dummy_vaccination(_sa_gdf):
    np.random.seed(42)
    
    return pd.DataFrame({
        "health_area": _sa_gdf["health_area"],
        "Taux_Vaccination": np.random.beta(a=8, b=2, size=len(_sa_gdf)) * 100
    })

# Chargement des donn√©es de cas
with st.spinner("üì• Chargement donn√©es de cas..."):
    if mode_demo == "üß™ Mode d√©mo (donn√©es simul√©es)":
        df = generate_dummy_linelists(sa_gdf, start=start_date, end=end_date)
        vaccination_df = generate_dummy_vaccination(sa_gdf)
        st.sidebar.info(f"üìä {len(df)} cas simul√©s g√©n√©r√©s")
        
    else:
        if linelist_file is None:
            st.error("‚ùå Veuillez uploader un fichier CSV de lineliste")
            st.stop()
            
        try:
            df_raw = pd.read_csv(linelist_file)
            
            if "Semaine_Epi" in df_raw.columns and "Cas_Total" in df_raw.columns:
                expanded_rows = []
                for _, row in df_raw.iterrows():
                    aire = row.get("health_area") or row.get("Aire_Sante") or row.get("name_fr")
                    semaine = int(row["Semaine_Epi"])
                    cas_total = int(row["Cas_Total"])
                    annee = row.get("Annee", 2024)
                    
                    base_date = datetime.strptime(f"{annee}-W{semaine:02d}-1", "%Y-W%W-%w")
                    
                    for i in range(cas_total):
                        expanded_rows.append({
                            "ID_Cas": len(expanded_rows) + 1,
                            "Date_Debut_Eruption": base_date + timedelta(days=np.random.randint(0, 7)),
                            "Date_Notification": base_date + timedelta(days=np.random.randint(0, 10)),
                            "Aire_Sante": aire,
                            "Age_Mois": 0,
                            "Statut_Vaccinal": "Inconnu",
                            "Sexe": "Inconnu",
                            "Issue": "Inconnu"
                        })
                
                df = pd.DataFrame(expanded_rows)
                
            elif "Date_Debut_Eruption" in df_raw.columns:
                df = df_raw.copy()
                
                for col in ["Date_Debut_Eruption", "Date_Notification"]:
                    if col in df.columns:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
            else:
                st.error("‚ùå Format CSV non reconnu")
                st.stop()
            
            st.sidebar.success(f"‚úì {len(df)} cas charg√©s")
            
        except Exception as e:
            st.error(f"‚ùå Erreur CSV : {e}")
            st.stop()
        
        if vaccination_file is not None:
            try:
                vaccination_df = pd.read_csv(vaccination_file)
                st.sidebar.success(f"‚úì Couverture vaccinale charg√©e ({len(vaccination_df)} aires)")
            except Exception as e:
                st.sidebar.warning(f"‚ö†Ô∏è Erreur vaccination CSV : {e}")
                vaccination_df = None
        else:
            if "Statut_Vaccinal" in df.columns:
                vacc_by_area = df.groupby("Aire_Sante").agg({
                    "Statut_Vaccinal": lambda x: ((x == "Oui").sum() / len(x) * 100) if len(x) > 0 else 0
                }).reset_index()
                vacc_by_area.columns = ["health_area", "Taux_Vaccination"]
                vaccination_df = vacc_by_area
                st.sidebar.info("‚ÑπÔ∏è Taux vaccination extrait de la linelist")
            else:
                vaccination_df = None
                st.sidebar.info("‚ÑπÔ∏è Pas de donn√©es de vaccination")

# Filtrer par p√©riode

# Normaliser les noms de colonnes du DataFrame df
COLONNES_MAPPING = {
    # Colonne aire de sant√©
    "Aire_Sante": ["Aire_Sante", "aire_sante", "health_area", "HEALTH_AREA", "name_fr", "NAME", "nom", "NOM"],
    # Colonne date de d√©but
    "Date_Debut_Eruption": ["Date_Debut_Eruption", "date_debut_eruption", "Date_Debut", "date_onset", "Date_Onset", "symptom_onset"],
    # Colonne date notification
    "Date_Notification": ["Date_Notification", "date_notification", "Date_Notif", "date_notif", "notification_date"],
    # Colonne ID cas
    "ID_Cas": ["ID_Cas", "id_cas", "ID", "id", "Case_ID", "case_id", "ID_cas"],
    # Colonne √¢ge
    "Age_Mois": ["Age_Mois", "age_mois", "Age", "age", "AGE", "Age_Months", "age_months"],
    # Colonne statut vaccinal
    "Statut_Vaccinal": ["Statut_Vaccinal", "statut_vaccinal", "Vaccin", "vaccin", "Vaccination_Status", "vaccination_status", "Vacc_Statut"],
    # Colonne sexe
    "Sexe": ["Sexe", "sexe", "Sex", "sex", "Gender", "gender"],
    # Colonne issue
    "Issue": ["Issue", "issue", "Outcome", "outcome", "OUTCOME"]
}

def normaliser_colonnes(dataframe, mapping):
    """Renommer les colonnes du dataframe selon le mapping standardis√©"""
    rename_dict = {}
    for col_standard, col_possibles in mapping.items():
        for col_possible in col_possibles:
            if col_possible in dataframe.columns and col_possible != col_standard:
                rename_dict[col_possible] = col_standard
                break
    if rename_dict:
        dataframe = dataframe.rename(columns=rename_dict)
    return dataframe

# Appliquer la normalisation
df = normaliser_colonnes(df, COLONNES_MAPPING)

# Si "ID_Cas" n'existe pas, en cr√©er une
if "ID_Cas" not in df.columns:
    df["ID_Cas"] = range(1, len(df) + 1)

# Si "Aire_Sante" n'existe pas, essayer de la cr√©er depuis sa_gdf
if "Aire_Sante" not in df.columns:
    # Chercher n'importe quelle colonne qui pourrait contenir un nom d'aire
    for col in df.columns:
        if df[col].dtype == object:
            # V√©rifier si les valeurs matchent avec les aires de sant√©
            sample_values = set(df[col].dropna().unique())
            sa_values = set(sa_gdf["health_area"].unique())
            if len(sample_values.intersection(sa_values)) > 0:
                df["Aire_Sante"] = df[col]
                st.sidebar.info(f"‚ÑπÔ∏è Colonne 'Aire_Sante' cr√©√©e depuis '{col}'")
                break
    else:
        # Si rien ne match, assigner une aire par d√©faut
        df["Aire_Sante"] = sa_gdf["health_area"].iloc[0]
        st.sidebar.warning("‚ö†Ô∏è Aucune colonne aire trouv√©e, valeur par d√©faut assign√©e")

# V√©rifier et convertir les dates
if "Date_Debut_Eruption" in df.columns:
    df["Date_Debut_Eruption"] = pd.to_datetime(df["Date_Debut_Eruption"], errors='coerce')
else:
    # Chercher une colonne date
    for col in df.columns:
        try:
            test_dates = pd.to_datetime(df[col], errors='coerce')
            if test_dates.notna().sum() > len(df) * 0.5:  # Plus de 50% de dates valides
                df["Date_Debut_Eruption"] = test_dates
                st.sidebar.info(f"‚ÑπÔ∏è 'Date_Debut_Eruption' cr√©√©e depuis '{col}'")
                break
        except:
            continue
    else:
        # Cr√©er une date par d√©faut
        df["Date_Debut_Eruption"] = pd.to_datetime(start_date)
        st.sidebar.warning("‚ö†Ô∏è Aucune colonne date trouv√©e, date de d√©but assign√©e par d√©faut")

if "Date_Notification" not in df.columns:
    # Cr√©er Date_Notification = Date_Debut_Eruption + 3 jours par d√©faut
    df["Date_Notification"] = df["Date_Debut_Eruption"] + pd.to_timedelta(3, unit="D")

# Ajouter des colonnes optionnelles par d√©faut si absentes
if "Age_Mois" not in df.columns:
    df["Age_Mois"] = np.nan

if "Statut_Vaccinal" not in df.columns:
    df["Statut_Vaccinal"] = "Inconnu"

if "Sexe" not in df.columns:
    df["Sexe"] = "Inconnu"

if "Issue" not in df.columns:
    df["Issue"] = "Inconnu"

# Filtrer par p√©riode
df = df[
    (df["Date_Debut_Eruption"] >= pd.to_datetime(start_date)) &
    (df["Date_Debut_Eruption"] <= pd.to_datetime(end_date))
].copy()

if len(df) == 0:
    st.warning("‚ö†Ô∏è Aucun cas dans la p√©riode")
    st.stop()

# Calculer semaine √©pid√©miologique
def calculer_semaine_epidemio(date):
    return date.isocalendar()[1]

df['Semaine_Epi'] = df['Date_Debut_Eruption'].apply(calculer_semaine_epidemio)
df['Annee'] = df['Date_Debut_Eruption'].dt.year
df['Semaine_Annee'] = df['Annee'].astype(str) + '-S' + df['Semaine_Epi'].astype(str).str.zfill(2)

derniere_semaine_epi = df['Semaine_Epi'].max()
derniere_annee = df['Annee'].max()

st.sidebar.info(f"üìÖ Derni√®re semaine : **S{derniere_semaine_epi}** ({derniere_annee})")
# ============================================================
# PARTIE 3/6 - ENRICHISSEMENT AVEC DONN√âES EXTERNES
# WorldPop, NASA POWER, GHSL
# ============================================================

# WorldPop - Donn√©es d√©mographiques
@st.cache_data
def worldpop_children_stats(_sa_gdf, use_gee):
    if not use_gee:
        st.sidebar.warning("‚ö†Ô∏è WorldPop : GEE indisponible")
        return pd.DataFrame({
            "health_area": _sa_gdf["health_area"],
            "Pop_Totale": [np.nan] * len(_sa_gdf),
            "Pop_Garcons": [np.nan] * len(_sa_gdf),
            "Pop_Filles": [np.nan] * len(_sa_gdf),
            "Pop_Enfants": [np.nan] * len(_sa_gdf),
            # Nouvelles colonnes pour pyramide
            "Pop_M_0": [np.nan] * len(_sa_gdf),
            "Pop_M_1": [np.nan] * len(_sa_gdf),
            "Pop_M_5": [np.nan] * len(_sa_gdf),
            "Pop_M_10": [np.nan] * len(_sa_gdf),
            "Pop_F_0": [np.nan] * len(_sa_gdf),
            "Pop_F_1": [np.nan] * len(_sa_gdf),
            "Pop_F_5": [np.nan] * len(_sa_gdf),
            "Pop_F_10": [np.nan] * len(_sa_gdf)
        })
    
    try:
        progress_bar = st.sidebar.progress(0)
        status_text = st.sidebar.empty()
        
        status_text.text("üì• Chargement WorldPop...")
        dataset = ee.ImageCollection("WorldPop/GP/100m/pop_age_sex")
        pop_img = dataset.mosaic()
        
        male_bands = ["M_0", "M_1", "M_5", "M_10"]
        female_bands = ["F_0", "F_1", "F_5", "F_10"]
        
        selected_males = pop_img.select(male_bands)
        selected_females = pop_img.select(female_bands)
        total_pop = pop_img.select(['population'])
        
        # Sommes par sexe
        males_sum = selected_males.reduce(ee.Reducer.sum()).rename('garcons')
        females_sum = selected_females.reduce(ee.Reducer.sum()).rename('filles')
        enfants = males_sum.add(females_sum).rename('enfants')
        
        # ========== MOSA√èQUE AVEC TOUTES LES BANDES ==========
        final_mosaic = (total_pop
                       .addBands(selected_males)      # Bandes M_0, M_1, M_5, M_10
                       .addBands(selected_females)    # Bandes F_0, F_1, F_5, F_10
                       .addBands(males_sum)
                       .addBands(females_sum)
                       .addBands(enfants))
        # ====================================================
        
        # Conversion densit√© ‚Üí compte absolu
        pixel_area = ee.Image.pixelArea().divide(10000)
        final_mosaic_count = final_mosaic.multiply(pixel_area)
        
        status_text.text("üó∫Ô∏è Conversion g√©om√©tries...")
        features = []
        for idx, row in _sa_gdf.iterrows():
            geom = row['geometry']
            props = {"health_area": row["health_area"]}
            
            if geom.geom_type == 'Polygon':
                coords = [[[x, y] for x, y in geom.exterior.coords]]
                ee_geom = ee.Geometry.Polygon(coords)
            elif geom.geom_type == 'MultiPolygon':
                coords = []
                for poly in geom.geoms:
                    coords.append([[[x, y] for x, y in poly.exterior.coords]])
                ee_geom = ee.Geometry.MultiPolygon(coords)
            else:
                continue
            
            features.append(ee.Feature(ee_geom, props))
        
        fc = ee.FeatureCollection(features)
        
        status_text.text("üî¢ Calcul statistiques zonales...")
        stats = final_mosaic_count.reduceRegions(
            collection=fc,
            reducer=ee.Reducer.sum(),
            scale=100,
            crs='EPSG:4326'
        )
        
        status_text.text("üìä Extraction r√©sultats...")
        stats_info = stats.getInfo()
        
        data_list = []
        total_aires = len(stats_info['features'])
        
        for i, feat in enumerate(stats_info['features']):
            props = feat['properties']
            
            # ========== EXTRACTION D√âTAILL√âE ==========
            pop_totale = props.get("population", 0)
            garcons = props.get("garcons", 0)
            filles = props.get("filles", 0)
            enfants_total = props.get("enfants", 0)
            
            # Extraire chaque tranche d'√¢ge individuellement
            m_0 = props.get("M_0", 0)
            m_1 = props.get("M_1", 0)
            m_5 = props.get("M_5", 0)
            m_10 = props.get("M_10", 0)
            
            f_0 = props.get("F_0", 0)
            f_1 = props.get("F_1", 0)
            f_5 = props.get("F_5", 0)
            f_10 = props.get("F_10", 0)
            # ==========================================
            
            data_list.append({
                "health_area": props.get("health_area", ""),
                "Pop_Totale": int(pop_totale) if pop_totale > 0 else np.nan,
                "Pop_Garcons": int(garcons),
                "Pop_Filles": int(filles),
                "Pop_Enfants": int(enfants_total),
                # Nouvelles colonnes pour pyramide
                "Pop_M_0": int(m_0),
                "Pop_M_1": int(m_1),
                "Pop_M_5": int(m_5),
                "Pop_M_10": int(m_10),
                "Pop_F_0": int(f_0),
                "Pop_F_1": int(f_1),
                "Pop_F_5": int(f_5),
                "Pop_F_10": int(f_10)
            })
            
            progress_value = min((i + 1) / total_aires, 1.0)
            progress_bar.progress(progress_value)
        
        progress_bar.empty()
        status_text.text("‚úÖ WorldPop termin√©")
        
        return pd.DataFrame(data_list)
        
    except Exception as e:
        st.sidebar.error(f"‚ùå WorldPop : {str(e)}")
        if 'progress_bar' in locals():
            progress_bar.empty()
        if 'status_text' in locals():
            status_text.empty()
        return pd.DataFrame({
            "health_area": _sa_gdf["health_area"],
            "Pop_Totale": [np.nan] * len(_sa_gdf),
            "Pop_Garcons": [np.nan] * len(_sa_gdf),
            "Pop_Filles": [np.nan] * len(_sa_gdf),
            "Pop_Enfants": [np.nan] * len(_sa_gdf),
            "Pop_M_0": [np.nan] * len(_sa_gdf),
            "Pop_M_1": [np.nan] * len(_sa_gdf),
            "Pop_M_5": [np.nan] * len(_sa_gdf),
            "Pop_M_10": [np.nan] * len(_sa_gdf),
            "Pop_F_0": [np.nan] * len(_sa_gdf),
            "Pop_F_1": [np.nan] * len(_sa_gdf),
            "Pop_F_5": [np.nan] * len(_sa_gdf),
            "Pop_F_10": [np.nan] * len(_sa_gdf)
        })


# GHSL - Classification urbaine
@st.cache_data
def urban_classification(_sa_gdf, use_gee):
    if not use_gee:
        st.sidebar.warning("‚ö†Ô∏è GHSL : GEE indisponible")
        return pd.DataFrame({
            "health_area": _sa_gdf["health_area"],
            "Urbanisation": [np.nan] * len(_sa_gdf)
        })
    
    try:
        progress_bar = st.sidebar.progress(0)
        status_text = st.sidebar.empty()
        status_text.text("üèôÔ∏è Classification urbaine...")
        
        features = []
        for idx, row in _sa_gdf.iterrows():
            geom = row['geometry']
            props = {"health_area": row["health_area"]}
            
            if geom.geom_type == 'Polygon':
                coords = [[[x, y] for x, y in geom.exterior.coords]]
                ee_geom = ee.Geometry.Polygon(coords)
            elif geom.geom_type == 'MultiPolygon':
                coords = []
                for poly in geom.geoms:
                    coords.append([[[x, y] for x, y in poly.exterior.coords]])
                ee_geom = ee.Geometry.MultiPolygon(coords)
            else:
                continue
            
            features.append(ee.Feature(ee_geom, props))
        
        fc = ee.FeatureCollection(features)
        smod = ee.Image("JRC/GHSL/P2023A/GHS_SMOD/2020")
        
        def classify(feature):
            stats = smod.reduceRegion(
                ee.Reducer.mode(),
                feature.geometry(),
                scale=1000,
                maxPixels=1e9
            )
            smod_value = ee.Number(stats.get("smod_code")).toInt()
            urbanisation = ee.Algorithms.If(
                smod_value.gte(30),
                "Urbain",
                ee.Algorithms.If(smod_value.eq(23), "Semi-urbain", "Rural")
            )
            return feature.set({"Urbanisation": urbanisation})
        
        urban_fc = fc.map(classify)
        urban_info = urban_fc.getInfo()
        
        data_list = []
        total_aires = len(urban_info['features'])
        
        for i, feat in enumerate(urban_info['features']):
            props = feat['properties']
            data_list.append({
                "health_area": props.get("health_area", ""),
                "Urbanisation": props.get("Urbanisation", "Rural")
            })
            progress_value = min((i + 1) / total_aires, 1.0)
            progress_bar.progress(progress_value)
        
        progress_bar.empty()
        status_text.text("‚úÖ GHSL termin√©")
        
        return pd.DataFrame(data_list)
        
    except Exception as e:
        st.sidebar.error(f"‚ùå GHSL : {str(e)}")
        if 'progress_bar' in locals():
            progress_bar.empty()
        if 'status_text' in locals():
            status_text.empty()
        return pd.DataFrame({
            "health_area": _sa_gdf["health_area"],
            "Urbanisation": [np.nan] * len(_sa_gdf)
        })

# NASA POWER - Donn√©es climatiques
@st.cache_data(ttl=86400)
def fetch_climate_nasa_power(_sa_gdf, start_date, end_date):
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    
    data_list = []
    total_aires = len(_sa_gdf)
    
    for idx, row in _sa_gdf.iterrows():
        status_text.text(f"üå°Ô∏è Climat {idx+1}/{total_aires}...")
        
        lat, lon = row.geometry.centroid.y, row.geometry.centroid.x
        
        url = "https://power.larc.nasa.gov/api/temporal/daily/point"
        params = {
            "parameters": "T2M,PRECTOTCORR,RH2M",
            "community": "AG",
            "longitude": lon,
            "latitude": lat,
            "start": start_date.strftime("%Y%m%d"),
            "end": end_date.strftime("%Y%m%d"),
            "format": "JSON"
        }
        
        try:
            r = requests.get(url, params=params, timeout=30)
            j = r.json()
            
            if "properties" in j and "parameter" in j["properties"]:
                p = j["properties"]["parameter"]
                
                temp_values = list(p.get("T2M", {}).values())
                rh_values = list(p.get("RH2M", {}).values())
                
                temp_mean = np.nanmean(temp_values) if temp_values else np.nan
                rh_mean = np.nanmean(rh_values) if rh_values else np.nan
                
                saison_seche_hum = rh_mean * 0.7 if not np.isnan(rh_mean) else np.nan
                
                data_list.append({
                    "health_area": row["health_area"],
                    "Temperature_Moy": temp_mean,
                    "Humidite_Moy": rh_mean,
                    "Saison_Seche_Humidite": saison_seche_hum
                })
            else:
                data_list.append({
                    "health_area": row["health_area"],
                    "Temperature_Moy": np.nan,
                    "Humidite_Moy": np.nan,
                    "Saison_Seche_Humidite": np.nan
                })
        except:
            data_list.append({
                "health_area": row["health_area"],
                "Temperature_Moy": np.nan,
                "Humidite_Moy": np.nan,
                "Saison_Seche_Humidite": np.nan
            })
        
        progress_value = min((idx + 1) / total_aires, 1.0)
        progress_bar.progress(progress_value)
    
    progress_bar.empty()
    status_text.text("‚úÖ Climat termin√©")
    
    return pd.DataFrame(data_list)

# Enrichissement du GeoDataFrame
with st.spinner("üîÑ Enrichissement des donn√©es..."):
    pop_df = worldpop_children_stats(sa_gdf, gee_ok)
    urban_df = urban_classification(sa_gdf, gee_ok)
    climate_df = fetch_climate_nasa_power(sa_gdf, start_date, end_date)

sa_gdf_enrichi = sa_gdf.copy()
sa_gdf_enrichi = sa_gdf_enrichi.merge(pop_df, on="health_area", how="left")
sa_gdf_enrichi = sa_gdf_enrichi.merge(urban_df, on="health_area", how="left")
sa_gdf_enrichi = sa_gdf_enrichi.merge(climate_df, on="health_area", how="left")

if vaccination_df is not None:
    sa_gdf_enrichi = sa_gdf_enrichi.merge(vaccination_df, on="health_area", how="left")
else:
    sa_gdf_enrichi["Taux_Vaccination"] = np.nan

# reprojection en m√©trique √©gale-aire
sa_gdf_m = sa_gdf_enrichi.to_crs("ESRI:54009")  # Mollweide (m√®tres)

sa_gdf_enrichi["Superficie_km2"] = sa_gdf_m.geometry.area / 1e6

sa_gdf_enrichi["Densite_Pop"] = (
    sa_gdf_enrichi["Pop_Totale"] / sa_gdf_enrichi["Superficie_km2"].replace(0, np.nan)
)

sa_gdf_enrichi["Densite_Enfants"] = (
    sa_gdf_enrichi["Pop_Enfants"] / sa_gdf_enrichi["Superficie_km2"].replace(0, np.nan)
)

sa_gdf_enrichi = sa_gdf_enrichi.replace([np.inf, -np.inf], np.nan)

st.sidebar.success("‚úì Enrichissement termin√©")

st.sidebar.markdown("---")
st.sidebar.subheader("üìã Donn√©es disponibles")

donnees_dispo = {
    "Population": not sa_gdf_enrichi["Pop_Totale"].isna().all(),
    "Urbanisation": not sa_gdf_enrichi["Urbanisation"].isna().all(),
    "Climat": not sa_gdf_enrichi["Humidite_Moy"].isna().all(),
    "Vaccination": not sa_gdf_enrichi["Taux_Vaccination"].isna().all()
}

for nom, dispo in donnees_dispo.items():
    icone = "‚úÖ" if dispo else "‚ùå"
    st.sidebar.text(f"{icone} {nom}")

# ============================================================
# PARTIE 4/6 - KPIS, CARTE ET ANALYSES
# ============================================================

# KPIs
st.header("üìä Indicateurs Cl√©s de Performance")

col1, col2, col3, col4, col5 = st.columns(5)

with col1:
    st.metric("üìà Cas totaux", f"{len(df):,}")

with col2:
    if "Statut_Vaccinal" in df.columns and df["Statut_Vaccinal"].notna().sum() > 0 and (df["Statut_Vaccinal"] != "Inconnu").sum() > 0:
        taux_non_vac = (df["Statut_Vaccinal"] == "Non").mean() * 100
        delta_vac = taux_non_vac - 45
        st.metric("üíâ Non vaccin√©s", f"{taux_non_vac:.1f}%", delta=f"{delta_vac:+.1f}%")
    else:
        st.metric("üíâ Non vaccin√©s", "N/A")

with col3:
    if "Age_Mois" in df.columns and df["Age_Mois"].notna().sum() > 0:
        age_median = df["Age_Mois"].median()
        st.metric("üë∂ √Çge m√©dian", f"{int(age_median)} mois")
    else:
        st.metric("üë∂ √Çge m√©dian", "N/A")

with col4:
    if "Issue" in df.columns and df["Issue"].notna().sum() > 0 and (df["Issue"] == "D√©c√©d√©").sum() > 0:
        taux_deces = (df["Issue"] == "D√©c√©d√©").mean() * 100
        st.metric("‚ò†Ô∏è L√©talit√©", f"{taux_deces:.2f}%")
    else:
        st.metric("‚ò†Ô∏è L√©talit√©", "N/A")

with col5:
    n_aires_touchees = df["Aire_Sante"].nunique()
    pct_aires = (n_aires_touchees / len(sa_gdf)) * 100
    st.metric("üó∫Ô∏è Aires touch√©es", f"{n_aires_touchees}/{len(sa_gdf)}", delta=f"{pct_aires:.0f}%")

# Agr√©gation par aire
agg_dict = {"ID_Cas": "count"}

if "Age_Mois" in df.columns:
    agg_dict["Age_Mois"] = "mean"

if "Statut_Vaccinal" in df.columns:
    agg_dict["Statut_Vaccinal"] = lambda x: (x == "Non").mean() * 100

cases_by_area = df.groupby("Aire_Sante").agg(agg_dict).reset_index()

# Renommer les colonnes selon ce qui est pr√©sent
rename_map = {"ID_Cas": "Cas_Observes"}
if "Age_Mois" in cases_by_area.columns:
    rename_map["Age_Mois"] = "Age_Moyen"
if "Statut_Vaccinal" in cases_by_area.columns:
    rename_map["Statut_Vaccinal"] = "Taux_Non_Vaccines"

cases_by_area = cases_by_area.rename(columns=rename_map)

# Ajouter des colonnes par d√©faut si absentes
if "Taux_Non_Vaccines" not in cases_by_area.columns:
    cases_by_area["Taux_Non_Vaccines"] = 0
if "Age_Moyen" not in cases_by_area.columns:
    cases_by_area["Age_Moyen"] = 0

cases_by_area.columns = ["Aire_Sante", "Cas_Observes", "Taux_Non_Vaccines", "Age_Moyen"]

sa_gdf_with_cases = sa_gdf_enrichi.merge(
    cases_by_area,
    left_on="health_area",
    right_on="Aire_Sante",
    how="left"
)

sa_gdf_with_cases["Cas_Observes"] = sa_gdf_with_cases["Cas_Observes"].fillna(0)
sa_gdf_with_cases["Taux_Non_Vaccines"] = sa_gdf_with_cases["Taux_Non_Vaccines"].fillna(0)

sa_gdf_with_cases["Taux_Attaque_10000"] = (
    sa_gdf_with_cases["Cas_Observes"] /
    sa_gdf_with_cases["Pop_Enfants"].replace(0, np.nan) * 10000
).replace([np.inf, -np.inf], np.nan)

# Carte de situation actuelle
st.header("üó∫Ô∏è Cartographie de la Situation Actuelle")

center_lat = sa_gdf_with_cases.geometry.centroid.y.mean()
center_lon = sa_gdf_with_cases.geometry.centroid.x.mean()

m = folium.Map(
    location=[center_lat, center_lon],
    zoom_start=6,
    tiles="CartoDB positron",
    control_scale=True
)

import branca.colormap as cm

max_cases = sa_gdf_with_cases["Cas_Observes"].max()

if max_cases > 0:
    colormap = cm.LinearColormap(
        colors=['#e8f5e9', '#81c784', '#ffeb3b', '#ff9800', '#f44336', '#b71c1c'],
        vmin=0,
        vmax=max_cases,
        caption="Nombre de cas observ√©s"
    )
    colormap.add_to(m)

for idx, row in sa_gdf_with_cases.iterrows():
    aire_name = row['health_area']
    cas_obs = int(row.get('Cas_Observes', 0))
    pop_enfants = row.get('Pop_Enfants', np.nan)
    taux_attaque = row.get('Taux_Attaque_10000', np.nan)
    urbanisation = row.get('Urbanisation', 'N/A')
    densite = row.get('Densite_Pop', np.nan)
    
    popup_html = f"""
    <div style="font-family: Arial; width: 350px;">
        <h3 style="margin-bottom: 10px; color: #1976d2; border-bottom: 2px solid #1976d2;">
            {aire_name}
        </h3>
        <div style="background-color: #f5f5f5; padding: 10px; margin: 10px 0; border-radius: 5px;">
            <h4 style="margin: 0; color: #d32f2f;">üìä Situation √âpid√©miologique</h4>
            <table style="width: 100%; margin-top: 5px;">
                <tr><td><b>Cas observ√©s :</b></td><td style="text-align: right;">
                    <b style="font-size: 18px; color: #d32f2f;">{cas_obs}</b>
                </td></tr>
                <tr><td>Population enfants :</td><td style="text-align: right;">
                    {f"{int(pop_enfants):,}" if not np.isnan(pop_enfants) else "N/A"}
                </td></tr>
                <tr><td>Taux d'attaque :</td><td style="text-align: right;">
                    {f"{taux_attaque:.1f}/10K" if not np.isnan(taux_attaque) else "N/A"}
                </td></tr>
                <tr><td>Type habitat :</td><td style="text-align: right;">
                    <b>{urbanisation if pd.notna(urbanisation) else "N/A"}</b>
                </td></tr>
                <tr><td>Densit√© pop :</td><td style="text-align: right;">
                    {f"{densite:.1f} hab/km¬≤" if not np.isnan(densite) else "N/A"}
                </td></tr>
            </table>
        </div>
    </div>
    """
    
    fill_color = colormap(row['Cas_Observes']) if max_cases > 0 else '#e0e0e0'
    
    if row['Cas_Observes'] >= seuil_alerte_epidemique:
        line_color = '#b71c1c'
        line_weight = 2
    else:
        line_color = 'black'
        line_weight = 0.5
    
    folium.GeoJson(
        row['geometry'],
        style_function=lambda x, color=fill_color, weight=line_weight, border=line_color: {
            'fillColor': color,
            'color': border,
            'weight': weight,
            'fillOpacity': 0.7
        },
        tooltip=folium.Tooltip(
            f"<b>{aire_name}</b><br>{cas_obs} cas",
            sticky=True
        ),
        popup=folium.Popup(popup_html, max_width=400)
    ).add_to(m)
    
    if cas_obs > 0:
        folium.Marker(
            location=[row.geometry.centroid.y, row.geometry.centroid.x],
            icon=folium.DivIcon(
                html=f"""
                <div style="
                    font-size: 7pt;
                    color: black;
                    font-weight: normal;
                    background: none;
                    padding: 0;
                    border: none;
                    box-shadow: none;
                    white-space: nowrap;
                ">{aire_name}</div>
                """
            ),
        ).add_to(m)


heat_data = [
    [row.geometry.centroid.y, row.geometry.centroid.x, row['Cas_Observes']]
    for idx, row in sa_gdf_with_cases.iterrows()
    if row['Cas_Observes'] > 0
]

if heat_data:
    HeatMap(
        heat_data,
        radius=20,
        blur=25,
        max_zoom=13,
        gradient={0.0: 'blue', 0.5: 'yellow', 1.0: 'red'}
    ).add_to(m)

legend_html = f'''
<div style="
    position: fixed;
    bottom: 50px;
    left: 50px;
    width: 250px;
    background-color: white;
    border: 2px solid grey;
    z-index:9999;
    font-size:14px;
    padding: 10px;
    border-radius: 5px;">
    <p style="margin: 0; font-weight: bold;">üìä L√©gende</p>
    <p style="margin: 5px 0;">
        <span style="background-color: #e8f5e9; padding: 2px 8px;">Faible</span>
        0-{max_cases//3:.0f} cas
    </p>
    <p style="margin: 5px 0;">
        <span style="background-color: #ffeb3b; padding: 2px 8px;">Moyen</span>
        {max_cases//3:.0f}-{2*max_cases//3:.0f} cas
    </p>
    <p style="margin: 5px 0;">
        <span style="background-color: #f44336; padding: 2px 8px; color: white;">√âlev√©</span>
        >{2*max_cases//3:.0f} cas
    </p>
    <p style="margin: 5px 0; padding-top: 5px; border-top: 1px solid #ccc;">
        <b>Seuil alerte :</b> {seuil_alerte_epidemique} cas/sem
    </p>
</div>
'''

m.get_root().html.add_child(folium.Element(legend_html))

st_folium(m, width=1400, height=650)

col1, col2, col3 = st.columns(3)

with col1:
    aires_alerte = len(sa_gdf_with_cases[sa_gdf_with_cases['Cas_Observes'] >= seuil_alerte_epidemique])
    st.metric("üö® Aires en alerte", aires_alerte, f"{aires_alerte/len(sa_gdf)*100:.1f}%")

with col2:
    aires_sans_cas = len(sa_gdf_with_cases[sa_gdf_with_cases['Cas_Observes'] == 0])
    st.metric("‚úÖ Aires sans cas", aires_sans_cas, f"{aires_sans_cas/len(sa_gdf)*100:.1f}%")

with col3:
    densite_pop_moy = sa_gdf_with_cases['Densite_Pop'].mean()
    st.metric("üìç Densit√© pop. moy.", f"{densite_pop_moy:.1f} hab/km¬≤")
# Analyse temporelle
st.header("üìà Analyse Temporelle par Semaines √âpid√©miologiques")

weekly_cases = df.groupby(['Annee', 'Semaine_Epi']).size().reset_index(name='Cas')
weekly_cases['Semaine_Label'] = weekly_cases['Annee'].astype(str) + '-S' + weekly_cases['Semaine_Epi'].astype(str).str.zfill(2)

fig_epi = go.Figure()

fig_epi.add_trace(go.Scatter(
    x=weekly_cases['Semaine_Label'],
    y=weekly_cases['Cas'],
    mode='lines+markers',
    name='Cas observ√©s',
    line=dict(color='#d32f2f', width=3),
    marker=dict(size=6),
    hovertemplate='<b>%{x}</b><br>Cas : %{y}<extra></extra>'
))

from scipy.signal import savgol_filter

if len(weekly_cases) > 5:
    tendance = savgol_filter(
        weekly_cases['Cas'],
        window_length=min(7, len(weekly_cases) if len(weekly_cases) % 2 == 1 else len(weekly_cases)-1),
        polyorder=2
    )
    fig_epi.add_trace(go.Scatter(
        x=weekly_cases['Semaine_Label'],
        y=tendance,
        mode='lines',
        name='Tendance',
        line=dict(color='#1976d2', width=2, dash='dash'),
        hovertemplate='<b>%{x}</b><br>Tendance : %{y:.1f}<extra></extra>'
    ))

fig_epi.add_hline(
    y=seuil_alerte_epidemique,
    line_dash="dot",
    line_color="orange",
    annotation_text=f"Seuil d'alerte ({seuil_alerte_epidemique} cas/sem)",
    annotation_position="right"
)

fig_epi.update_layout(
    title="Courbe √©pid√©mique par semaines √©pid√©miologiques",
    xaxis_title="Semaine √©pid√©miologique",
    yaxis_title="Nombre de cas",
    hovermode='x unified',
    height=400
)

st.plotly_chart(fig_epi, use_container_width=True)

col1, col2, col3 = st.columns(3)

with col1:
    semaine_max = weekly_cases.loc[weekly_cases['Cas'].idxmax()]
    st.metric(
        "üî¥ Semaine avec pic maximal",
        semaine_max['Semaine_Label'],
        f"{int(semaine_max['Cas'])} cas"
    )

with col2:
    cas_moyen_semaine = weekly_cases['Cas'].mean()
    st.metric("üìä Moyenne hebdomadaire", f"{cas_moyen_semaine:.1f} cas")

with col3:
    if len(weekly_cases) >= 2:
        variation = weekly_cases.iloc[-1]['Cas'] - weekly_cases.iloc[-2]['Cas']
        cas_precedent = weekly_cases.iloc[-2]['Cas']
        pct_variation = (variation / cas_precedent * 100) if cas_precedent > 0 else 0
        st.metric("üìâ Variation derni√®re semaine", f"{int(variation):+d} cas", f"{pct_variation:+.1f}%")
    else:
        st.metric("üìâ Variation derni√®re semaine", "N/A")

# Distribution par √¢ge
st.subheader("üë∂ Distribution par Tranches d'√Çge")

if "Age_Mois" in df.columns:
    df["Tranche_Age"] = pd.cut(
        df["Age_Mois"],
        bins=[0, 12, 60, 120, 180],
        labels=["0-1 an", "1-5 ans", "5-10 ans", "10-15 ans"]
    )
    age_available = True
else:
    df["Tranche_Age"] = "Inconnu"
    age_available = False

agg_dict_age = {"ID_Cas": "count"}

if "Statut_Vaccinal" in df.columns:
    agg_dict_age["Statut_Vaccinal"] = lambda x: (x == "Non").mean() * 100

age_stats = df.groupby("Tranche_Age").agg(agg_dict_age).reset_index()

rename_map_age = {"ID_Cas": "Nombre_Cas"}
if "Statut_Vaccinal" in age_stats.columns:
    rename_map_age["Statut_Vaccinal"] = "Pct_Non_Vaccines"

age_stats = age_stats.rename(columns=rename_map_age)

if "Pct_Non_Vaccines" not in age_stats.columns:
    age_stats["Pct_Non_Vaccines"] = 0

col1, col2 = st.columns(2)

col1, col2 = st.columns(2)

with col1:
    if age_available:
        fig_age = px.bar(
            age_stats,
            x="Tranche_Age",
            y="Nombre_Cas",
            title="Cas par tranche d'√¢ge",
            color="Nombre_Cas",
            color_continuous_scale="Reds",
            text="Nombre_Cas"
        )
        fig_age.update_traces(textposition='outside')
        st.plotly_chart(fig_age, use_container_width=True)
    else:
        st.info("‚ÑπÔ∏è Donn√©es d'√¢ge non disponibles")

with col2:
    if age_available and age_stats["Pct_Non_Vaccines"].sum() > 0:
        fig_vacc_age = px.bar(
            age_stats,
            x="Tranche_Age",
            y="Pct_Non_Vaccines",
            title="% non vaccin√©s par √¢ge",
            color="Pct_Non_Vaccines",
            color_continuous_scale="Oranges",
            text="Pct_Non_Vaccines"
        )
        fig_vacc_age.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
        st.plotly_chart(fig_vacc_age, use_container_width=True)
    else:
        st.info("‚ÑπÔ∏è Donn√©es de vaccination par √¢ge non disponibles")
# ============================================================
# PYRAMIDE DES √ÇGES POPULATION (WORLDPOP)
# ============================================================

st.header("üìä Pyramide des √Çges - Population Enfantine")

if donnees_dispo["Population"]:
    # Pr√©parer les donn√©es de population par tranche d'√¢ge
    pyramid_data = []
    
    for idx, row in sa_gdf_enrichi.iterrows():
        aire = row['health_area']
        
        # R√©cup√©rer les valeurs de population par tranche d'√¢ge depuis WorldPop
        # Si vous avez d√©j√† extrait ces donn√©es individuellement
        pop_0_1_m = row.get('Pop_M_0', 0) + row.get('Pop_M_1', 0)  # 0-4 ans gar√ßons
        pop_5_9_m = row.get('Pop_M_5', 0)  # 5-9 ans gar√ßons
        pop_10_14_m = row.get('Pop_M_10', 0)  # 10-14 ans gar√ßons
        
        pop_0_1_f = row.get('Pop_F_0', 0) + row.get('Pop_F_1', 0)  # 0-4 ans filles
        pop_5_9_f = row.get('Pop_F_5', 0)  # 5-9 ans filles
        pop_10_14_f = row.get('Pop_F_10', 0)  # 10-14 ans filles
        
        pyramid_data.append({
            'Aire': aire,
            'Gar√ßons_0-4': pop_0_1_m,
            'Gar√ßons_5-9': pop_5_9_m,
            'Gar√ßons_10-14': pop_10_14_m,
            'Filles_0-4': pop_0_1_f,
            'Filles_5-9': pop_5_9_f,
            'Filles_10-14': pop_10_14_f
        })
    
    pyramid_df = pd.DataFrame(pyramid_data)
    
    # Agr√©ger pour tout le territoire
    total_garcons_0_4 = pyramid_df['Gar√ßons_0-4'].sum()
    total_garcons_5_9 = pyramid_df['Gar√ßons_5-9'].sum()
    total_garcons_10_14 = pyramid_df['Gar√ßons_10-14'].sum()
    
    total_filles_0_4 = pyramid_df['Filles_0-4'].sum()
    total_filles_5_9 = pyramid_df['Filles_5-9'].sum()
    total_filles_10_14 = pyramid_df['Filles_10-14'].sum()
    
    # Cr√©er le dataframe pour la pyramide
    age_groups = ['0-4', '5-9', '10-14']
    
    pyramid_plot_df = pd.DataFrame({
        'Age': age_groups,
        'Gar√ßons': [-total_garcons_0_4, -total_garcons_5_9, -total_garcons_10_14],  # N√©gatif pour gauche
        'Filles': [total_filles_0_4, total_filles_5_9, total_filles_10_14]  # Positif pour droite
    })
    
    # Cr√©er la pyramide avec Plotly
    fig_pyramid = go.Figure()
    
    # Barres gar√ßons (gauche - n√©gatif)
    fig_pyramid.add_trace(go.Bar(
        y=pyramid_plot_df['Age'],
        x=pyramid_plot_df['Gar√ßons'],
        name='Gar√ßons',
        orientation='h',
        marker=dict(color='#42a5f5'),
        text=[f"{abs(x):,.0f}" for x in pyramid_plot_df['Gar√ßons']],
        textposition='inside',
        hovertemplate='<b>%{y} ans</b><br>Gar√ßons: %{text}<extra></extra>'
    ))
    
    # Barres filles (droite - positif)
    fig_pyramid.add_trace(go.Bar(
        y=pyramid_plot_df['Age'],
        x=pyramid_plot_df['Filles'],
        name='Filles',
        orientation='h',
        marker=dict(color='#ec407a'),
        text=[f"{x:,.0f}" for x in pyramid_plot_df['Filles']],
        textposition='inside',
        hovertemplate='<b>%{y} ans</b><br>Filles: %{text}<extra></extra>'
    ))
    
    # Calculer les limites sym√©triques
    max_val = max(
        abs(pyramid_plot_df['Gar√ßons'].min()),
        pyramid_plot_df['Filles'].max()
    )
    
    fig_pyramid.update_layout(
        title='Pyramide des √Çges - Population Enfantine (0-14 ans)',
        xaxis=dict(
            title='Population',
            tickvals=[-max_val, -max_val/2, 0, max_val/2, max_val],
            ticktext=[f"{int(max_val):,}", f"{int(max_val/2):,}", "0", 
                     f"{int(max_val/2):,}", f"{int(max_val):,}"],
            range=[-max_val * 1.1, max_val * 1.1]
        ),
        yaxis=dict(title='Tranche d\'√¢ge'),
        barmode='overlay',
        height=400,
        bargap=0.1,
        showlegend=True,
        legend=dict(x=0.85, y=0.95),
        hovermode='y unified'
    )
    
    st.plotly_chart(fig_pyramid, use_container_width=True)
    
    # Statistiques compl√©mentaires
    col1, col2, col3 = st.columns(3)
    
    with col1:
        total_garcons = total_garcons_0_4 + total_garcons_5_9 + total_garcons_10_14
        st.metric("üë¶ Gar√ßons (0-14 ans)", f"{int(total_garcons):,}")
    
    with col2:
        total_filles = total_filles_0_4 + total_filles_5_9 + total_filles_10_14
        st.metric("üëß Filles (0-14 ans)", f"{int(total_filles):,}")
    
    with col3:
        ratio = (total_garcons / total_filles * 100) if total_filles > 0 else 0
        st.metric("‚öñÔ∏è Ratio G/F", f"{ratio:.1f}%")

else:
    st.info("üìä Donn√©es de population non disponibles. Pyramide des √¢ges non affichable.")

# Nowcasting
st.subheader("‚è±Ô∏è Nowcasting - Correction des D√©lais de Notification")

st.info("""
**Nowcasting (Pr√©vision imm√©diate) :** Technique d'ajustement permettant d'estimer le nombre r√©el de cas en tenant compte des d√©lais de notification.
""")

if "Date_Notification" in df.columns and "Date_Debut_Eruption" in df.columns:
    df["Delai_Notification"] = (df["Date_Notification"] - df["Date_Debut_Eruption"]).dt.days
    delai_available = True
else:
    df["Delai_Notification"] = 3  # Valeur par d√©faut
    delai_available = False

delai_moyen = df["Delai_Notification"].mean()
delai_median = df["Delai_Notification"].median()
delai_std = df["Delai_Notification"].std()

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("D√©lai moyen de notification", f"{delai_moyen:.1f} jours" if delai_available and not np.isnan(delai_moyen) else "N/A")

with col2:
    st.metric("D√©lai m√©dian", f"{delai_median:.0f} jours" if delai_available and not np.isnan(delai_median) else "N/A")

with col3:
    st.metric("√âcart-type", f"{delai_std:.1f} jours" if delai_available and not np.isnan(delai_std) else "N/A")

with col4:
    derniere_semaine_label = weekly_cases.iloc[-1]['Semaine_Label']
    cas_derniere_semaine = int(weekly_cases.iloc[-1]['Cas'])
    if delai_available and not np.isnan(delai_moyen):
        facteur_correction = 1 + (delai_moyen / 7)
        cas_corriges = int(cas_derniere_semaine * facteur_correction)
        st.metric(
            f"Cas corrig√©s ({derniere_semaine_label})",
            cas_corriges,
            delta=f"+{cas_corriges - cas_derniere_semaine}"
        )
    else:
        st.metric(
            f"Cas corrig√©s ({derniere_semaine_label})",
            cas_derniere_semaine,
            delta="N/A"
        )

if delai_available:
    fig_delai = px.histogram(
        df,
        x="Delai_Notification",
        nbins=20,
        title="Distribution des d√©lais de notification",
        labels={"Delai_Notification": "D√©lai (jours)", "count": "Nombre de cas"},
        color_discrete_sequence=['#d32f2f']
    )

    fig_delai.add_vline(x=delai_moyen, line_dash="dash", line_color="blue", annotation_text=f"Moyenne : {delai_moyen:.1f}j")
    fig_delai.add_vline(x=delai_median, line_dash="dash", line_color="green", annotation_text=f"M√©diane : {delai_median:.0f}j")

    st.plotly_chart(fig_delai, use_container_width=True)
else:
    st.info("‚ÑπÔ∏è Donn√©es de d√©lai de notification non disponibles")

# ============================================================
# PARTIE 5/6 - MOD√âLISATION PR√âDICTIVE AVEC SYST√àME HYBRIDE
# ============================================================

st.header("üîÆ Mod√©lisation Pr√©dictive par Semaines √âpid√©miologiques")

st.markdown(f"""
<div class="info-box">
<b>Configuration de la pr√©diction :</b><br>
- Derni√®re semaine de donn√©es : <b>S{derniere_semaine_epi} ({derniere_annee})</b><br>
- P√©riode de pr√©diction : <b>{pred_mois} mois ({n_weeks_pred} semaines)</b><br>
- Semaines pr√©dites : <b>S{derniere_semaine_epi+1} √† S{min(derniere_semaine_epi+n_weeks_pred, 52)}</b><br>
- Mod√®le s√©lectionn√© : <b>{modele_choisi}</b><br>
- Mode importance : <b>{mode_importance}</b><br>
- Seuils configur√©s : Baisse ‚â•{seuil_baisse}%, Hausse ‚â•{seuil_hausse}%
</div>
""", unsafe_allow_html=True)

if 'prediction_lancee' not in st.session_state:
    st.session_state.prediction_lancee = False

col1, col2 = st.columns([3, 1])

with col1:
    if st.button("üöÄ Lancer la Mod√©lisation Pr√©dictive", type="primary", use_container_width=True):
        st.session_state.prediction_lancee = True
        st.rerun()

with col2:
    if st.button("üîÑ R√©initialiser", use_container_width=True):
        st.session_state.prediction_lancee = False
        st.rerun()

if st.session_state.prediction_lancee:
    
    with st.spinner("ü§ñ Pr√©paration des donn√©es et entra√Ænement..."):
        
        weekly_features = df.groupby(["Aire_Sante", "Annee", "Semaine_Epi"]).agg(
            Cas_Observes=("ID_Cas", "count"),
            Non_Vaccines=("Statut_Vaccinal", lambda x: (x == "Non").mean() * 100),
            Age_Moyen=("Age_Mois", "mean")
        ).reset_index()
        
        weekly_features['Semaine_Label'] = (
            weekly_features['Annee'].astype(str) + '-S' +
            weekly_features['Semaine_Epi'].astype(str).str.zfill(2)
        )
        
        weekly_features = weekly_features.merge(
            sa_gdf_enrichi[[
                "health_area", "Pop_Totale", "Pop_Enfants",
                "Densite_Pop", "Densite_Enfants", "Urbanisation",
                "Temperature_Moy", "Humidite_Moy", "Saison_Seche_Humidite",
                "Taux_Vaccination"
            ]],
            left_on="Aire_Sante",
            right_on="health_area",
            how="left"
        )
        
        le_urban = LabelEncoder()
        weekly_features["Urban_Encoded"] = le_urban.fit_transform(
            weekly_features["Urbanisation"].fillna("Rural")
        )
        
        # Cr√©er un coefficient climatique composite si donn√©es disponibles
        if donnees_dispo["Climat"]:
            scaler_climat = MinMaxScaler()
            climate_cols = ["Temperature_Moy", "Humidite_Moy", "Saison_Seche_Humidite"]
            
            for col in climate_cols:
                if col in weekly_features.columns:
                    weekly_features[f"{col}_Norm"] = scaler_climat.fit_transform(
                        weekly_features[[col]].fillna(weekly_features[col].mean())
                    )
            
            weekly_features["Coef_Climatique"] = (
                weekly_features.get("Temperature_Moy_Norm", 0) * 0.4 +
                weekly_features.get("Humidite_Moy_Norm", 0) * 0.4 +
                weekly_features.get("Saison_Seche_Humidite_Norm", 0) * 0.2
            )
        
        weekly_features = weekly_features.sort_values(['Aire_Sante', 'Annee', 'Semaine_Epi'])
        
        for lag in [1, 2, 3, 4]:
            weekly_features[f'Cas_Lag_{lag}'] = (
                weekly_features.groupby('Aire_Sante')['Cas_Observes'].shift(lag)
            )
        
        numeric_cols = weekly_features.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            weekly_features[col] = weekly_features[col].replace([np.inf, -np.inf], 0)
            mean_val = weekly_features[col].mean()
            weekly_features[col] = weekly_features[col].fillna(mean_val if pd.notna(mean_val) else 0)
        
        st.subheader("üìö Entra√Ænement du Mod√®le")
        
        # Construire la liste des features en fonction des donn√©es disponibles
        feature_cols = [
            "Cas_Observes", "Age_Moyen", "Semaine_Epi",
            "Cas_Lag_1", "Cas_Lag_2", "Cas_Lag_3", "Cas_Lag_4"
        ]
        
        # Mapping des groupes vers les colonnes individuelles
        feature_groups = {
            "Historique_Cas": ["Cas_Lag_1", "Cas_Lag_2", "Cas_Lag_3", "Cas_Lag_4"],
            "Vaccination": [],
            "Demographie": [],
            "Urbanisation": [],
            "Climat": []
        }
        
        if donnees_dispo["Population"]:
            feature_cols.extend(["Pop_Totale", "Pop_Enfants", "Densite_Pop", "Densite_Enfants"])
            feature_groups["Demographie"] = ["Pop_Totale", "Pop_Enfants", "Densite_Pop", "Densite_Enfants"]
            st.info("‚úÖ Donn√©es d√©mographiques int√©gr√©es au mod√®le")
        
        if donnees_dispo["Urbanisation"]:
            feature_cols.append("Urban_Encoded")
            feature_groups["Urbanisation"] = ["Urban_Encoded"]
            st.info("‚úÖ Classification urbaine int√©gr√©e au mod√®le")
        
        if donnees_dispo["Climat"]:
            feature_cols.append("Coef_Climatique")
            feature_groups["Climat"] = ["Coef_Climatique"]
            st.info("‚úÖ Coefficient climatique composite int√©gr√© au mod√®le")
        
        if donnees_dispo["Vaccination"]:
            feature_cols.extend(["Taux_Vaccination", "Non_Vaccines"])
            feature_groups["Vaccination"] = ["Taux_Vaccination", "Non_Vaccines"]
            st.info("‚úÖ Donn√©es vaccinales int√©gr√©es au mod√®le")
        elif "Non_Vaccines" in weekly_features.columns:
            feature_cols.append("Non_Vaccines")
            feature_groups["Vaccination"] = ["Non_Vaccines"]
        
        st.markdown(f"**Variables utilis√©es :** {len(feature_cols)} features")
        
        weekly_features_clean = weekly_features.dropna(subset=feature_cols)
        
        if len(weekly_features_clean) < 20:
            st.warning("‚ö†Ô∏è Donn√©es insuffisantes (minimum 20 observations requises)")
            st.stop()
        
        X = weekly_features_clean[feature_cols].copy()
        y = weekly_features_clean["Cas_Observes"]
        
        # ========== APPLICATION DES POIDS (MODE MANUEL) ==========
        if mode_importance == "üë®‚Äç‚öïÔ∏è Manuel (Expert)":
            st.markdown('<div class="weight-box">', unsafe_allow_html=True)
            st.markdown("**‚öñÔ∏è Application des poids manuels aux variables**")
            
            # Cr√©er un dictionnaire de mapping colonne ‚Üí poids
            column_weights = {}
            
            for group_name, weight in poids_normalises.items():
                if group_name in feature_groups:
                    cols_in_group = feature_groups[group_name]
                    if len(cols_in_group) > 0:
                        weight_per_col = weight / len(cols_in_group)
                        for col in cols_in_group:
                            if col in feature_cols:
                                column_weights[col] = weight_per_col
            
            # Ajouter des poids par d√©faut pour les colonnes non group√©es
            for col in feature_cols:
                if col not in column_weights:
                    column_weights[col] = 0.01
            
            # Appliquer les poids aux features
            X_weighted = X.copy()
            for col in feature_cols:
                if col in column_weights:
                    X_weighted[col] = X_weighted[col] * column_weights[col]
            
            # Afficher le d√©tail des poids appliqu√©s
            weights_df = pd.DataFrame({
                "Variable": list(column_weights.keys()),
                "Poids": [f"{v*100:.2f}%" for v in column_weights.values()]
            })
            
            col1, col2 = st.columns([2, 1])
            with col1:
                st.dataframe(weights_df, use_container_width=True, hide_index=True)
            with col2:
                st.metric("Total des poids", "100.00%")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Utiliser X_weighted pour l'entra√Ænement
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_weighted)
        else:
            # Mode automatique : pas de pond√©ration manuelle
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        if modele_choisi == "GradientBoosting (Recommand√©)":
            model = GradientBoostingRegressor(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=5,
                min_samples_split=4,
                random_state=42
            )
        elif modele_choisi == "RandomForest":
            model = RandomForestRegressor(
                n_estimators=200,
                max_depth=10,
                min_samples_split=4,
                random_state=42
            )
        elif modele_choisi == "Ridge Regression":
            model = Ridge(alpha=1.0, random_state=42)
        elif modele_choisi == "Lasso Regression":
            model = Lasso(alpha=0.1, random_state=42)
        elif modele_choisi == "Decision Tree":
            model = DecisionTreeRegressor(
                max_depth=8,
                min_samples_split=4,
                random_state=42
            )
        
        model.fit(X_train, y_train)
        
        score_test = model.score(X_test, y_test)
        cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üìä R¬≤ Test", f"{score_test:.3f}")
        with col2:
            st.metric("üéØ R¬≤ CV (5-fold)", f"{cv_mean:.3f}")
        with col3:
            st.metric("üìè √âcart-type CV", f"¬±{cv_std:.3f}")
        
        if cv_mean > 0.7:
            st.success(f"‚úÖ Mod√®le performant ({modele_choisi})")
        elif cv_mean > 0.5:
            st.warning(f"‚ö†Ô∏è Mod√®le acceptable ({modele_choisi})")
        else:
            st.error(f"‚ùå Mod√®le peu performant - envisagez un autre algorithme")
        
        # Afficher l'importance des variables (automatique OU ajust√©e)
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                "Variable": feature_cols,
                "Importance": model.feature_importances_
            }).sort_values("Importance", ascending=False)
            
            with st.expander("üìä Importance des variables", expanded=True):
                if mode_importance == "üë®‚Äç‚öïÔ∏è Manuel (Expert)":
                    st.info("‚ÑπÔ∏è Ces importances refl√®tent l'influence des variables **apr√®s application des poids manuels**")
                else:
                    st.info("‚ÑπÔ∏è Ces importances sont **calcul√©es automatiquement** par le mod√®le ML")
                
                fig_imp = px.bar(
                    feature_importance.head(10),
                    x="Importance",
                    y="Variable",
                    orientation="h",
                    title="Top 10 variables les plus importantes",
                    color="Importance",
                    color_continuous_scale="Viridis"
                )
                st.plotly_chart(fig_imp, use_container_width=True)
        
        st.subheader(f"üìÖ G√©n√©ration des Pr√©dictions - {n_weeks_pred} Semaines")
        
        future_climate = None
        if donnees_dispo["Climat"]:
            future_start = end_date + timedelta(days=1)
            future_end = end_date + timedelta(days=n_weeks_pred * 7)
            
            with st.spinner("üå°Ô∏è Chargement pr√©visions climatiques..."):
                try:
                    future_climate = fetch_climate_nasa_power(sa_gdf, future_start, future_end)
                    st.info("‚úÖ Pr√©visions climatiques int√©gr√©es aux pr√©dictions")
                except:
                    st.warning("‚ö†Ô∏è Pr√©visions climatiques indisponibles - utilisation valeurs moyennes")
        
        future_predictions = []
        
        for aire in weekly_features["Aire_Sante"].unique():
            aire_data = weekly_features[weekly_features["Aire_Sante"] == aire].sort_values(['Annee', 'Semaine_Epi'])
            
            if aire_data.empty:
                continue
            
            last_obs = aire_data.iloc[-1]
            
            last_4_weeks = aire_data.tail(4)['Cas_Observes'].values
            if len(last_4_weeks) < 4:
                last_4_weeks = np.pad(last_4_weeks, (4-len(last_4_weeks), 0), 'edge')
            
            for i in range(1, n_weeks_pred + 1):
                nouvelle_semaine_epi = (derniere_semaine_epi + i - 1) % 52 + 1
                nouvelle_annee = derniere_annee + ((derniere_semaine_epi + i - 1) // 52)
                
                future_row = {
                    "Aire_Sante": aire,
                    "Annee": nouvelle_annee,
                    "Semaine_Epi": nouvelle_semaine_epi,
                    "Semaine_Label": f"{nouvelle_annee}-S{str(nouvelle_semaine_epi).zfill(2)}",
                    "Age_Moyen": last_obs["Age_Moyen"]
                }
                
                if donnees_dispo["Population"]:
                    future_row.update({
                        "Pop_Totale": last_obs["Pop_Totale"],
                        "Pop_Enfants": last_obs["Pop_Enfants"],
                        "Densite_Pop": last_obs["Densite_Pop"],
                        "Densite_Enfants": last_obs["Densite_Enfants"]
                    })
                
                if donnees_dispo["Urbanisation"]:
                    future_row["Urban_Encoded"] = last_obs["Urban_Encoded"]
                
                if donnees_dispo["Climat"]:
                    if future_climate is not None:
                        climate_aire = future_climate[future_climate["health_area"] == aire]
                        if not climate_aire.empty:
                            temp_norm = scaler_climat.transform([[climate_aire.iloc[0]["Temperature_Moy"]]])[0][0]
                            hum_norm = scaler_climat.transform([[climate_aire.iloc[0]["Humidite_Moy"]]])[0][0]
                            saison_norm = scaler_climat.transform([[climate_aire.iloc[0]["Saison_Seche_Humidite"]]])[0][0]
                            
                            future_row["Coef_Climatique"] = temp_norm * 0.4 + hum_norm * 0.4 + saison_norm * 0.2
                        else:
                            future_row["Coef_Climatique"] = last_obs.get("Coef_Climatique", 0)
                    else:
                        future_row["Coef_Climatique"] = last_obs.get("Coef_Climatique", 0)
                
                if donnees_dispo["Vaccination"]:
                    future_row["Taux_Vaccination"] = last_obs["Taux_Vaccination"]
                    future_row["Non_Vaccines"] = last_obs["Non_Vaccines"]
                elif "Non_Vaccines" in last_obs:
                    future_row["Non_Vaccines"] = last_obs["Non_Vaccines"]
                
                if i == 1:
                    future_row["Cas_Observes"] = last_obs["Cas_Observes"]
                    future_row["Cas_Lag_1"] = last_4_weeks[-1]
                    future_row["Cas_Lag_2"] = last_4_weeks[-2] if len(last_4_weeks) >= 2 else last_4_weeks[-1]
                    future_row["Cas_Lag_3"] = last_4_weeks[-3] if len(last_4_weeks) >= 3 else last_4_weeks[-1]
                    future_row["Cas_Lag_4"] = last_4_weeks[-4] if len(last_4_weeks) >= 4 else last_4_weeks[-1]
                else:
                    prev_predictions = [
                        p["Predicted_Cases"] for p in future_predictions
                        if p["Aire_Sante"] == aire
                    ]
                    future_row["Cas_Observes"] = prev_predictions[-1] if prev_predictions else last_obs["Cas_Observes"]
                    future_row["Cas_Lag_1"] = prev_predictions[-1] if len(prev_predictions) >= 1 else last_4_weeks[-1]
                    future_row["Cas_Lag_2"] = prev_predictions[-2] if len(prev_predictions) >= 2 else last_4_weeks[-2]
                    future_row["Cas_Lag_3"] = prev_predictions[-3] if len(prev_predictions) >= 3 else last_4_weeks[-3]
                    future_row["Cas_Lag_4"] = prev_predictions[-4] if len(prev_predictions) >= 4 else last_4_weeks[-4]
                
                X_future = np.array([[future_row[col] for col in feature_cols]])
                
                # Appliquer les m√™mes poids si mode manuel
                if mode_importance == "üë®‚Äç‚öïÔ∏è Manuel (Expert)":
                    for idx, col in enumerate(feature_cols):
                        if col in column_weights:
                            X_future[0, idx] = X_future[0, idx] * column_weights[col]
                
                X_future_scaled = scaler.transform(X_future)
                
                predicted_cases = max(0, model.predict(X_future_scaled)[0])
                
                if cv_std > 0:
                    noise = np.random.normal(0, predicted_cases * cv_std * 0.1)
                    predicted_cases = max(0, predicted_cases + noise)
                
                future_row["Predicted_Cases"] = predicted_cases
                future_predictions.append(future_row)
        
        future_df = pd.DataFrame(future_predictions)
        
        st.success(f"‚úì {len(future_df)} pr√©dictions g√©n√©r√©es ({len(future_df['Aire_Sante'].unique())} aires √ó {n_weeks_pred} semaines)")
        
        moyenne_historique = weekly_features.groupby("Aire_Sante")["Cas_Observes"].mean().reset_index()
        moyenne_historique.columns = ["Aire_Sante", "Moyenne_Historique"]
        
        risk_df = future_df.groupby("Aire_Sante").agg(
            Cas_Predits_Total=("Predicted_Cases", "sum"),
            Cas_Predits_Max=("Predicted_Cases", "max"),
            Cas_Predits_Moyen=("Predicted_Cases", "mean"),
            Semaine_Pic=("Predicted_Cases", lambda x: future_df.loc[x.idxmax(), "Semaine_Label"] if len(x) > 0 else "N/A")
        ).reset_index()
        
        risk_df = risk_df.merge(moyenne_historique, on="Aire_Sante", how="left")
        
        risk_df["Variation_Pct"] = (
            (risk_df["Cas_Predits_Moyen"] - risk_df["Moyenne_Historique"]) /
            risk_df["Moyenne_Historique"].replace(0, 1)
        ) * 100
        
        risk_df["Categorie_Variation"] = pd.cut(
            risk_df["Variation_Pct"],
            bins=[-np.inf, -seuil_baisse, -10, 10, seuil_hausse, np.inf],
            labels=["Forte baisse", "Baisse mod√©r√©e", "Stable", "Hausse mod√©r√©e", "Forte hausse"]
        )
        
        risk_df = risk_df.sort_values("Variation_Pct", ascending=False)

# ============================================================
# PARTIE 6/6 - VISUALISATIONS FINALES ET EXPORTS
# ============================================================

        st.subheader("üìä Synth√®se des Pr√©dictions")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_predits = risk_df["Cas_Predits_Total"].sum()
            st.metric("Total cas pr√©dits", f"{int(total_predits):,}")
        
        with col2:
            aires_hausse = len(risk_df[risk_df["Variation_Pct"] >= seuil_hausse])
            st.metric("üî¥ Aires en hausse", aires_hausse, f"‚â•{seuil_hausse}%")
        
        with col3:
            aires_baisse = len(risk_df[risk_df["Variation_Pct"] <= -seuil_baisse])
            st.metric("üü¢ Aires en baisse", aires_baisse, f"‚â•{seuil_baisse}%")
        
        with col4:
            aires_stables = len(risk_df[(risk_df["Variation_Pct"] > -10) & (risk_df["Variation_Pct"] < 10)])
            st.metric("‚ö™ Aires stables", aires_stables, "¬±10%")
        
        fig_var_dist = px.pie(
            risk_df,
            names="Categorie_Variation",
            title="Distribution des tendances pr√©dites",
            color="Categorie_Variation",
            color_discrete_map={
                "Forte baisse": "#2e7d32",
                "Baisse mod√©r√©e": "#81c784",
                "Stable": "#9e9e9e",
                "Hausse mod√©r√©e": "#ff9800",
                "Forte hausse": "#d32f2f"
            }
        )
        st.plotly_chart(fig_var_dist, use_container_width=True)
        
        st.subheader("üî• Heatmap Temporelle des Pr√©dictions")
        
        top_10_hausse = risk_df.head(10)["Aire_Sante"].tolist()
        top_10_baisse = risk_df.tail(10)["Aire_Sante"].tolist()
        
        heatmap_data = future_df.pivot(
            index="Aire_Sante",
            columns="Semaine_Label",
            values="Predicted_Cases"
        ).fillna(0)
        
        col_heat1, col_heat2 = st.columns(2)
        
        with col_heat1:
            st.markdown("**üî¥ Top 10 - Hausses pr√©dites**")
            heatmap_hausse = heatmap_data.loc[heatmap_data.index.isin(top_10_hausse)]
            
            fig_heat_hausse = px.imshow(
                heatmap_hausse,
                labels=dict(x="Semaine", y="Aire", color="Cas"),
                x=heatmap_hausse.columns,
                y=heatmap_hausse.index,
                color_continuous_scale=["#ffebee", "#ffcdd2", "#ef9a9a", "#e57373", "#ef5350", "#f44336", "#d32f2f"],
                aspect="auto"
            )
            fig_heat_hausse.update_xaxes(side="bottom", tickangle=-45)
            fig_heat_hausse.update_layout(height=400)
            st.plotly_chart(fig_heat_hausse, use_container_width=True)
        
        with col_heat2:
            st.markdown("**üü¢ Top 10 - Baisses pr√©dites**")
            heatmap_baisse = heatmap_data.loc[heatmap_data.index.isin(top_10_baisse)]
            
            fig_heat_baisse = px.imshow(
                heatmap_baisse,
                labels=dict(x="Semaine", y="Aire", color="Cas"),
                x=heatmap_baisse.columns,
                y=heatmap_baisse.index,
                color_continuous_scale=["#e8f5e9", "#c8e6c9", "#a5d6a7", "#81c784", "#66bb6a", "#4caf50", "#2e7d32"],
                aspect="auto"
            )
            fig_heat_baisse.update_xaxes(side="bottom", tickangle=-45)
            fig_heat_baisse.update_layout(height=400)
            st.plotly_chart(fig_heat_baisse, use_container_width=True)
        
        tab1, tab2, tab3 = st.tabs([
            f"üî¥ Hausse ‚â•{seuil_hausse}%",
            f"üü¢ Baisse ‚â•{seuil_baisse}%",
            "üìä Toutes les aires"
        ])
        
        with tab1:
            st.subheader(f"Aires avec Hausse Significative (‚â•{seuil_hausse}%)")
            hausse_df = risk_df[risk_df["Variation_Pct"] >= seuil_hausse].copy()
            
            if len(hausse_df) > 0:
                def highlight_hausse(row):
                    return ["background-color: #ffcdd2"] * len(row)
                
                st.dataframe(
                    hausse_df[[
                        "Aire_Sante", "Moyenne_Historique", "Cas_Predits_Moyen",
                        "Variation_Pct", "Semaine_Pic", "Cas_Predits_Max"
                    ]].style.apply(highlight_hausse, axis=1).format({
                        "Moyenne_Historique": "{:.1f}",
                        "Cas_Predits_Moyen": "{:.1f}",
                        "Variation_Pct": "{:+.1f}%",
                        "Cas_Predits_Max": "{:.0f}"
                    }),
                    use_container_width=True
                )
                
                st.warning(f"‚ö†Ô∏è **{len(hausse_df)} aire(s)** n√©cessite(nt) une vigilance accrue")
            else:
                st.success("‚úì Aucune aire avec hausse significative")
        
        with tab2:
            st.subheader(f"Aires avec Baisse Significative (‚â•{seuil_baisse}%)")
            baisse_df = risk_df[risk_df["Variation_Pct"] <= -seuil_baisse].copy()
            
            if len(baisse_df) > 0:
                def highlight_baisse(row):
                    return ["background-color: #c8e6c9"] * len(row)
                
                st.dataframe(
                    baisse_df[[
                        "Aire_Sante", "Moyenne_Historique", "Cas_Predits_Moyen",
                        "Variation_Pct", "Semaine_Pic", "Cas_Predits_Max"
                    ]].style.apply(highlight_baisse, axis=1).format({
                        "Moyenne_Historique": "{:.1f}",
                        "Cas_Predits_Moyen": "{:.1f}",
                        "Variation_Pct": "{:+.1f}%",
                        "Cas_Predits_Max": "{:.0f}"
                    }),
                    use_container_width=True
                )
                
                st.success(f"‚úì **{len(baisse_df)} aire(s)** montre(nt) une am√©lioration")
            else:
                st.info("‚ÑπÔ∏è Aucune aire avec baisse significative")
        
        with tab3:
            st.subheader("Tableau Complet des Pr√©dictions")
            
            def highlight_all(row):
                if row["Variation_Pct"] >= seuil_hausse:
                    return ["background-color: #ffcdd2"] * len(row)
                elif row["Variation_Pct"] <= -seuil_baisse:
                    return ["background-color: #c8e6c9"] * len(row)
                else:
                    return [""] * len(row)
            
            st.dataframe(
                risk_df[[
                    "Aire_Sante", "Moyenne_Historique", "Cas_Predits_Moyen",
                    "Variation_Pct", "Categorie_Variation", "Semaine_Pic", "Cas_Predits_Max"
                ]].style.apply(highlight_all, axis=1).format({
                    "Moyenne_Historique": "{:.1f}",
                    "Cas_Predits_Moyen": "{:.1f}",
                    "Variation_Pct": "{:+.1f}%",
                    "Cas_Predits_Max": "{:.0f}"
                }),
                use_container_width=True,
                height=400
            )
        
        st.subheader("üó∫Ô∏è Carte des Pr√©dictions")
        
        sa_gdf_pred = sa_gdf_enrichi.merge(
            risk_df,
            left_on="health_area",
            right_on="Aire_Sante",
            how="left"
        )
        
        sa_gdf_pred["Variation_Pct"] = sa_gdf_pred["Variation_Pct"].fillna(0)
        sa_gdf_pred["Cas_Predits_Max"] = sa_gdf_pred["Cas_Predits_Max"].fillna(0)
        
        m_pred = folium.Map(
            location=[center_lat, center_lon],
            zoom_start=6,
            tiles="CartoDB positron"
        )
        
        max_var = max(abs(sa_gdf_pred["Variation_Pct"].min()), abs(sa_gdf_pred["Variation_Pct"].max()))
        
        colormap_pred = cm.LinearColormap(
            colors=['#2e7d32', '#81c784', '#e0e0e0', '#ff9800', '#d32f2f'],
            vmin=-max_var,
            vmax=max_var,
            caption="Variation (%) par rapport √† la moyenne"
        )
        colormap_pred.add_to(m_pred)
        
        for idx, row in sa_gdf_pred.iterrows():
            aire_name = row.get('health_area', 'Aire inconnue')
            variation_pct = row.get('Variation_Pct', 0)
            moy_historique = row.get('Moyenne_Historique', 0)
            cas_pred_moy = row.get('Cas_Predits_Moyen', 0)
            cas_pred_max = row.get('Cas_Predits_Max', 0)
            semaine_pic = row.get('Semaine_Pic', 'N/A')
            categorie = row.get('Categorie_Variation', 'N/A')
            
            if variation_pct >= seuil_hausse:
                bg_color = '#ffebee'
                var_color = '#d32f2f'
            elif variation_pct <= -seuil_baisse:
                bg_color = '#e8f5e9'
                var_color = '#2e7d32'
            else:
                bg_color = '#f5f5f5'
                var_color = '#000'
            
            popup_html = f"""
            <div style="font-family: Arial; width: 360px;">
                <h3 style="color: #1976d2; border-bottom: 2px solid #1976d2;">
                    {aire_name}
                </h3>
                <div style="background-color: {bg_color}; padding: 10px; margin: 10px 0; border-radius: 5px;">
                    <h4 style="margin: 0;">üîÆ Pr√©dictions</h4>
                    <table style="width: 100%; margin-top: 5px;">
                        <tr><td><b>Moyenne historique :</b></td><td style="text-align: right;">
                            {moy_historique:.1f} cas/sem
                        </td></tr>
                        <tr><td><b>Moyenne pr√©dite :</b></td><td style="text-align: right;">
                            {cas_pred_moy:.1f} cas/sem
                        </td></tr>
                        <tr><td><b>Variation :</b></td><td style="text-align: right; font-size: 18px; color: {var_color};">
                            <b>{variation_pct:+.1f}%</b>
                        </td></tr>
                        <tr><td>Tendance :</td><td style="text-align: right;">
                            <b>{categorie}</b>
                        </td></tr>
                        <tr><td>Semaine du pic :</td><td style="text-align: right;">
                            {semaine_pic}
                        </td></tr>
                        <tr><td>Pic maximal :</td><td style="text-align: right;">
                            {int(cas_pred_max)} cas
                        </td></tr>
                    </table>
                </div>
            </div>
            """
            
            fill_color = colormap_pred(variation_pct) if pd.notna(variation_pct) else '#e0e0e0'
            
            folium.GeoJson(
                row['geometry'],
                style_function=lambda x, color=fill_color: {
                    'fillColor': color,
                    'color': 'black',
                    'weight': 0.5,
                    'fillOpacity': 0.7
                },
                tooltip=folium.Tooltip(
                    f"<b>{aire_name}</b><br>Variation : {variation_pct:+.1f}%",
                    sticky=True
                ),
                popup=folium.Popup(popup_html, max_width=400)
            ).add_to(m_pred)
        
        st_folium(m_pred, width=1400, height=650)
        
        st.header("üíæ Export des R√©sultats")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            csv_pred = risk_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="üì• Synth√®se pr√©dictions (CSV)",
                data=csv_pred,
                file_name=f"predictions_synthese_{pays_selectionne if pays_selectionne else 'pays'}_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
        
        with col2:
            csv_detail = future_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="üìÖ D√©tail par semaine (CSV)",
                data=csv_detail,
                file_name=f"predictions_detail_{pays_selectionne if pays_selectionne else 'pays'}_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
        
        with col3:
            geojson_str = sa_gdf_pred.to_json()
            st.download_button(
                label="üó∫Ô∏è Carte pr√©dictions (GeoJSON)",
                data=geojson_str,
                file_name=f"carte_predictions_{pays_selectionne if pays_selectionne else 'pays'}_{datetime.now().strftime('%Y%m%d')}.geojson",
                mime="application/json"
            )
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            risk_df.to_excel(writer, sheet_name='Synthese', index=False)
            future_df.to_excel(writer, sheet_name='Detail_Semaines', index=False)
            cases_by_area.to_excel(writer, sheet_name='Cas_Observes', index=False)
            age_stats.to_excel(writer, sheet_name='Analyse_Age', index=False)
            weekly_cases.to_excel(writer, sheet_name='Historique_Hebdo', index=False)
        
        st.download_button(
            label="üìä Rapport complet (Excel)",
            data=output.getvalue(),
            file_name=f"rapport_complet_{pays_selectionne if pays_selectionne else 'pays'}_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        st.header("üí° Recommandations Op√©rationnelles")
        
        aires_critiques_hausse = risk_df[risk_df["Variation_Pct"] >= seuil_hausse]["Aire_Sante"].tolist()
        aires_amelioration = risk_df[risk_df["Variation_Pct"] <= -seuil_baisse]["Aire_Sante"].tolist()
        
        if aires_critiques_hausse:
            st.error(f"üö® **{len(aires_critiques_hausse)} aire(s) √† risque CRITIQUE (hausse ‚â•{seuil_hausse}%)**")
            
            for i, aire in enumerate(aires_critiques_hausse[:5], 1):
                aire_info = risk_df[risk_df["Aire_Sante"] == aire].iloc[0]
                st.write(f"{i}. **{aire}** : {aire_info['Variation_Pct']:+.1f}% - Pic S{aire_info['Semaine_Pic']} ({int(aire_info['Cas_Predits_Max'])} cas)")
            
            if len(aires_critiques_hausse) > 5:
                st.caption(f"... et {len(aires_critiques_hausse)-5} autre(s)")
            
            st.write("")
            st.write("**Actions prioritaires recommand√©es :**")
            st.write("‚úÖ Renforcer la surveillance √©pid√©miologique hebdomadaire dans ces aires")
            st.write("‚úÖ Organiser des campagnes de vaccination de rattrapage urgentes")
            st.write("‚úÖ Pr√©positionner les stocks de vaccins et intrants m√©dicaux")
            st.write("‚úÖ Sensibiliser les communaut√©s aux signes d'alerte de la rougeole")
            st.write("‚úÖ Coordonner avec les partenaires (OMS, UNICEF, MSF)")
            st.write("‚úÖ Pr√©parer les structures de sant√© √† une augmentation de cas")
        
        if aires_amelioration:
            st.success(f"‚úì **{len(aires_amelioration)} aire(s) montrent une am√©lioration (baisse ‚â•{seuil_baisse}%)**")
            st.write("**Bonnes pratiques √† capitaliser :**")
            st.write("‚Ä¢ Documenter les interventions ayant conduit √† cette baisse")
            st.write("‚Ä¢ Partager les le√ßons apprises avec les autres aires")
            st.write("‚Ä¢ Maintenir la vigilance pour √©viter une r√©surgence")
        
        if not aires_critiques_hausse and not aires_amelioration:
            st.info("‚ÑπÔ∏è **Situation stable** - Aucune variation significative d√©tect√©e")
            st.write("‚Ä¢ Maintenir la surveillance de routine")
            st.write("‚Ä¢ Continuer les activit√©s de vaccination selon le calendrier")
        
        st.markdown("---")
        st.caption(f"""
**M√©thodologie de pr√©diction :**
Mod√®le : {modele_choisi} | Score R¬≤ (validation crois√©e) : {cv_mean:.3f} (¬±{cv_std:.3f}) |
Variables : {len(feature_cols)} features (historique 4 semaines, d√©mographie, urbanisation, climat, vaccination) |
P√©riode : S{derniere_semaine_epi+1} √† S{min(derniere_semaine_epi+n_weeks_pred, 52)} ({n_weeks_pred} semaines) |
Seuils : Baisse ‚â•{seuil_baisse}%, Hausse ‚â•{seuil_hausse}%, Alerte ‚â•{seuil_alerte_epidemique} cas/sem
        """)

else:
    st.info("üëÜ Cliquez sur le bouton ci-dessus pour lancer la mod√©lisation pr√©dictive")
    st.markdown("""

    """)

st.markdown("---")
st.caption(f"""
Dashboard de Surveillance Rougeole - Version 3.0 (Am√©lior√©e) |
G√©n√©r√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M')} |
Pays : {pays_selectionne if pays_selectionne else 'Non sp√©cifi√©'} |
P√©riode : {start_date} - {end_date} |
Nombre d'aires : {len(sa_gdf)} |
Cas analys√©s : {len(df):,} |
Mode : {mode_demo}
""")
